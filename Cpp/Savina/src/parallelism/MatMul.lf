/**
 * Copyright (C) 2020 TU Dresden
 * 
 * This benchmarks implements a parallel matrix multiplication algorithm.
 * Overall, the benchmark uses a divide-and-conquer approach that is very
 * similar to the N Queens benchmark. The Manager at first produces a WorkItem
 * that encompasses the whole problem, and the workers then split the work items
 * into smaller chunks that they send back to the manager. Only if the problem
 * size is below a threshold, the workers perform an actual multiplication over
 * a subset of the matrix. When the problem is split, always 8 new work items
 * are generated.
 * 
 * It is important to note that the Savina implementation makes a severe
 * mistake. Both the operand matrices and the result matrix are stored in shared
 * memory. For the operands, this is not much of an issue since they are
 * read-only. However, the matrix is written to by all workers using an
 * accumulate (read-write) operation. Since multiple workers can write to the
 * same cell simultaneously, we have a race condition. This makes the result
 * nondeterministic. The problem is illustrated nicely by the Akka
 * implementation which, on my machine and in the default configuration, prints
 * "valid: false". Due to the nature of race conditions, this might be hard to
 * reproduce on other machines. However, I found that when we change the problem
 * size, the error appears or disappears sometimes.
 *
 * This implementation in LF reproduces the mistake described above. This is to
 * replicate the same behaviour. If we would only fix the LF variant, this would
 * make a comparison to the Akka version hard. This is of course less than
 * ideal...
 * 
 * @author Christian Menard
 * @author Hannes Klein
 */

target Cpp {
    build-type : Release,
    no-runtime-validation: true,
    cmake-include : "../IncludeHeaders.cmake",
    logging: "warn"
};

import BenchmarkRunner from "../BenchmarkRunner.lf";

public preamble {=
    #include <deque>
    #include <cmath>
    #include "Matrix.hh"

    template<class T>
    class TransposedMatrix {
    private:
        std::vector<T> data;
        size_t size_x;

    public:
        TransposedMatrix(size_t size_x, size_t size_y) : data(size_x * size_y), size_x(size_x) {}

        const T& at(size_t x, size_t y) const { return data[y*size_x+x]; }
        T& at(size_t x, size_t y) { return data[y*size_x+x]; }
    };
    
    struct WorkItem {
        size_t srA; // srA = start row in matrix A
        size_t scA; // scA = start column in matrix A
        size_t srB;
        size_t scB;
        size_t srC;
        size_t scC;
        size_t numBlocks; // total number of elements per block in both dimensions
        size_t dim; // number of elements in one dimension in one block
    };
=}

reactor Manager(numWorkers: size_t{20}, dataLength: size_t{1024}) {
    state A: Matrix<double>(dataLength, dataLength);
    state B: TransposedMatrix<double>(dataLength, dataLength);
    state C: Matrix<double>(dataLength, dataLength);
    
    state workQueue: std::deque<{=reactor::ImmutableValuePtr<WorkItem>=}>;
    
    logical action next;
    logical action done;
    
    input start: void;
    output finished: void;
    
    output data: {=std::tuple<const Matrix<double>*, const TransposedMatrix<double>*, Matrix<double>*>=};
    output[numWorkers] doWork: WorkItem;
    input[numWorkers] moreWork: {=std::array<reactor::ImmutableValuePtr<WorkItem>, 8>=};
   
    reaction (startup) {=
        // Fill both input arrays with data
        for (size_t i{0}; i < dataLength; ++i) {
            for (size_t j{0}; j < dataLength; ++j) {
                A.at(i, j) = i;
                B.at(i, j) = j;
            }
        }
    =}
    
    reaction (start) -> data, next {=
        // reset the result matrix C
        C = Matrix<double>(dataLength, dataLength);

        // send pointers to all 3 matrixes to the workers
        data.set(std::make_tuple(&A, &B, &C));
        
        // produce the first work item, instructing the worker to multiply the complete matrix
        size_t numBlocks = dataLength * dataLength;
        auto item = reactor::make_immutable_value<WorkItem>(WorkItem{0, 0, 0, 0, 0, 0, numBlocks, dataLength});
        workQueue.emplace_back(std::move(item));
        // and start the first iteration
        next.schedule();
    =}
    
    reaction (next) -> next, done, doWork {=
        if (workQueue.empty()) {
            // we are done if there is no more work
            done.schedule();
        } else {
            // send a work item to each worker (until there is no more work)
            for (size_t i{0}; i < numWorkers && !workQueue.empty(); i++) {
                doWork[i].set(workQueue.front());
                workQueue.pop_front();
            }
            // and schedule the next iteration
            next.schedule();
        }
    =}
    
    reaction (moreWork) {=
        // append all work items received from the workers to the internal work queue
        for (auto i: moreWork.present_indices_unsortedes()) {
            const auto& items = *moreWork[i].get();
            if (!items.empty()) {
                workQueue.insert(workQueue.end(), items.begin(), items.end());
            }
        }
    =}
    
    reaction (done) -> finished {=
        bool valid = isValid();
        reactor::log::Info() << std::boolalpha << "Result valid = " << valid << std::noboolalpha;
        finished.set();
    =}
    
    const method isValid(): bool {=
        for (size_t i{0}; i < dataLength; i++) {
            for (size_t j{0}; j < dataLength; j++) {
                double actual = C.at(i, j);
                double expected = 1.0 * dataLength * i * j;
                if (fabs(actual-expected) > 0.0001) { // allow some rounding errors
                    reactor::log::Info() << "Validation failed for (i,j)=" << i << "," << j << " with (" << actual << "," << expected << ")";
                    return false;    
                }
            }
        }
        return true;
    =}
}

reactor Worker(threshold: size_t{16384}) {

    state A: {=const Matrix<double>*=};
    state B: {=const TransposedMatrix<double>*=};
    state C: {=Matrix<double>*=};
    
    input data: {=std::tuple<const Matrix<double>*, const TransposedMatrix<double>*, Matrix<double>*>=};
    input doWork: WorkItem;    
    output moreWork: {=std::array<reactor::ImmutableValuePtr<WorkItem>, 8>=};
    
    reaction (data) {=
        const auto& tuple = *data.get();
        A = std::get<0>(tuple);
        B = std::get<1>(tuple);
        C = std::get<2>(tuple);
    =}
    
    reaction(doWork) -> moreWork {=
        const WorkItem& wi = *doWork.get();
        
        // If the number of blocks to process is above the threshold,
        // then we split the problem into smaller chunks and generate more work items
        if (wi.numBlocks > threshold) {
            auto workQueue = reactor::make_mutable_value<std::array<reactor::ImmutableValuePtr<WorkItem>, 8>>();            

            size_t dim = wi.dim / 2;
            size_t numBlocks = wi.numBlocks / 4;
            
            workQueue->at(0) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA      , wi.scA      , wi.srB      , wi.scB      , wi.srC      , wi.scC      , numBlocks, dim});
            workQueue->at(1) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA      , wi.scA + dim, wi.srB + dim, wi.scB      , wi.srC      , wi.scC      , numBlocks, dim});
            workQueue->at(2) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA      , wi.scA      , wi.srB      , wi.scB + dim, wi.srC      , wi.scC + dim, numBlocks, dim});
            workQueue->at(3) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA      , wi.scA + dim, wi.srB + dim, wi.scB + dim, wi.srC      , wi.scC + dim, numBlocks, dim});
            workQueue->at(4) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA + dim, wi.scA      , wi.srB      , wi.scB      , wi.srC + dim, wi.scC      , numBlocks, dim});
            workQueue->at(5) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA + dim, wi.scA + dim, wi.srB + dim, wi.scB      , wi.srC + dim, wi.scC      , numBlocks, dim});
            workQueue->at(6) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA + dim, wi.scA      , wi.srB      , wi.scB + dim, wi.srC + dim, wi.scC + dim, numBlocks, dim});
            workQueue->at(7) = reactor::make_immutable_value<WorkItem>(WorkItem{wi.srA + dim, wi.scA + dim, wi.srB + dim, wi.scB + dim, wi.srC + dim, wi.scC + dim, numBlocks, dim});

            moreWork.set(std::move(workQueue));
        } else {
            // otherwise we compute the result directly
            size_t endR{wi.srC + wi.dim};
            size_t endC{wi.scC + wi.dim};
        
            for (size_t i{wi.srC}; i < endR; i++) {
                for (size_t j{wi.scC}; j < endC; j++) {
                    for (size_t k{0}; k < wi.dim; k++) {
                        C->at(i, j) += A->at(i, wi.scA + k) * B->at(wi.srB + k, j);
                    }
                }
            }
        }
    =}
}

main reactor (numIterations: size_t{12}, dataLength: size_t{1024}, blockThreshold: size_t{16384}, priorities:size_t{10}, numWorkers: size_t{20}) {
    // The priorities parameter is not used, but we keep it for compatibility with Akka
    runner = new BenchmarkRunner(numIterations=numIterations);
    manager = new Manager(numWorkers=numWorkers, dataLength=dataLength);
    workers = new[numWorkers] Worker(threshold=blockThreshold) 
    
    reaction(startup) {=        
        printBenchmarkInfo("MatMulBenchmark");
        printArgs("numIterations", numIterations, "dataLength", dataLength, "blockThreshold", blockThreshold, "priorities", priorities, "numWorkers", numWorkers);
        printSystemInfo();
    =}
    
    runner.start -> manager.start;
    manager.finished -> runner.finished;
    
    (manager.data)+ -> workers.data;
    manager.doWork -> workers.doWork;
    workers.moreWork -> manager.moreWork;
}
